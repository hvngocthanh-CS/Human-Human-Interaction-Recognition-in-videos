{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-28T14:11:10.855412Z",
     "iopub.status.busy": "2025-03-28T14:11:10.855111Z",
     "iopub.status.idle": "2025-03-28T14:11:13.985949Z",
     "shell.execute_reply": "2025-03-28T14:11:13.984597Z",
     "shell.execute_reply.started": "2025-03-28T14:11:10.855390Z"
    },
    "id": "LINZXw1k1slG",
    "outputId": "96b232ec-c56c-4ee7-8dba-b9eafa9b9de4",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: decord in /usr/local/lib/python3.10/dist-packages (0.6.0)\n",
      "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from decord) (1.26.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install decord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-28T14:13:03.174061Z",
     "iopub.status.busy": "2025-03-28T14:13:03.173686Z",
     "iopub.status.idle": "2025-03-28T14:13:06.330760Z",
     "shell.execute_reply": "2025-03-28T14:13:06.329745Z",
     "shell.execute_reply.started": "2025-03-28T14:13:03.174030Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: einops in /usr/local/lib/python3.10/dist-packages (0.8.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install einops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-28T14:13:16.260245Z",
     "iopub.status.busy": "2025-03-28T14:13:16.259882Z",
     "iopub.status.idle": "2025-03-28T14:13:19.410577Z",
     "shell.execute_reply": "2025-03-28T14:13:19.409739Z",
     "shell.execute_reply.started": "2025-03-28T14:13:16.260222Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.44.2)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.24.7)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.5)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.6.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-28T14:16:44.242816Z",
     "iopub.status.busy": "2025-03-28T14:16:44.242465Z",
     "iopub.status.idle": "2025-03-28T14:16:49.301124Z",
     "shell.execute_reply": "2025-03-28T14:16:49.300103Z",
     "shell.execute_reply.started": "2025-03-28T14:16:44.242788Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pysrt\n",
      "  Downloading pysrt-1.1.2.tar.gz (104 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.4/104.4 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: chardet in /usr/local/lib/python3.10/dist-packages (from pysrt) (5.2.0)\n",
      "Building wheels for collected packages: pysrt\n",
      "  Building wheel for pysrt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for pysrt: filename=pysrt-1.1.2-py3-none-any.whl size=13443 sha256=614e8989756229fcdf63c129a5d7299d959b90558a834226a96bfd8dde95c913\n",
      "  Stored in directory: /root/.cache/pip/wheels/30/7f/e8/55de9a9b07302d9e7fe47c27910e3bea0c48536153e74bd7e6\n",
      "Successfully built pysrt\n",
      "Installing collected packages: pysrt\n",
      "Successfully installed pysrt-1.1.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pysrt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-28T14:16:49.303024Z",
     "iopub.status.busy": "2025-03-28T14:16:49.302675Z",
     "iopub.status.idle": "2025-03-28T14:16:55.378537Z",
     "shell.execute_reply": "2025-03-28T14:16:55.377584Z",
     "shell.execute_reply.started": "2025-03-28T14:16:49.302994Z"
    },
    "id": "Uo_GvV45pEdM",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "import torch.nn.functional as F\n",
    "import pysrt\n",
    "import re\n",
    "import pickle\n",
    "import math\n",
    "\n",
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from decord import VideoReader, cpu\n",
    "from torch.utils.data import DataLoader, Dataset, TensorDataset, random_split\n",
    "from torchvision import transforms\n",
    "import torchaudio\n",
    "import random\n",
    "import torch.optim as optim\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, average_precision_score\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from einops import rearrange, repeat, reduce\n",
    "from einops.layers.torch import Rearrange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-28T14:16:55.464436Z",
     "iopub.status.busy": "2025-03-28T14:16:55.463966Z",
     "iopub.status.idle": "2025-03-28T14:16:55.524835Z",
     "shell.execute_reply": "2025-03-28T14:16:55.523895Z",
     "shell.execute_reply.started": "2025-03-28T14:16:55.464409Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def read_data(video_dir, text_dir, audio_dir, output_file):\n",
    "    data = {}\n",
    "\n",
    "    for main_subfolder in os.listdir(video_dir):\n",
    "        video_main_path = os.path.join(video_dir, main_subfolder)\n",
    "        text_main_path = os.path.join(text_dir, main_subfolder)\n",
    "        audio_main_path = os.path.join(audio_dir, main_subfolder)\n",
    "\n",
    "        # Kiểm tra nếu folder chính tồn tại ở cả video, text và audio\n",
    "        if os.path.isdir(video_main_path) and os.path.isdir(text_main_path) and os.path.isdir(audio_main_path):\n",
    "            # Duyệt qua các subfolder (subfolderA1, subfolderA2, ...)\n",
    "            for subfolder in os.listdir(video_main_path):\n",
    "                video_subfolder_path = os.path.join(video_main_path, subfolder)\n",
    "                text_subfolder_path = os.path.join(text_main_path, subfolder)\n",
    "                audio_subfolder_path = os.path.join(audio_main_path, subfolder)\n",
    "\n",
    "                # Kiểm tra nếu subfolder tồn tại ở cả video, text và audio\n",
    "                if os.path.isdir(video_subfolder_path) and os.path.isdir(text_subfolder_path) and os.path.isdir(audio_subfolder_path):\n",
    "                    # Duyệt qua các file .mp4 trong subfolder video\n",
    "                    for file in os.listdir(video_subfolder_path):\n",
    "                        if file.endswith('.mp4'):\n",
    "                            base_name = os.path.splitext(file)[0]  # Tên file không có đuôi\n",
    "                            video_path = os.path.join(video_subfolder_path, file)\n",
    "                            text_path = os.path.join(text_subfolder_path, f\"{base_name}.srt\")\n",
    "                            audio_path = os.path.join(audio_subfolder_path, f\"{base_name}.mp3\")\n",
    "\n",
    "                            # Kiểm tra các file .srt và .mp3 tương ứng\n",
    "                            if os.path.exists(text_path) and os.path.exists(audio_path):\n",
    "                                data[base_name] = {\"video\": video_path, \"text\": text_path, \"audio\": audio_path}\n",
    "                            else:\n",
    "                                print(f\"Warning: Missing text or audio file for {file}\")\n",
    "\n",
    "    # Lưu dữ liệu bằng pickle\n",
    "    with open(output_file, \"wb\") as f:\n",
    "        pickle.dump(data, f)\n",
    "    print(f\"Data saved to {output_file}\")\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "video_train = \"/kaggle/input/full-dataset/clip_train\" \n",
    "text_train = \"/kaggle/input/full-dataset/dialogue_train\"\n",
    "audio_train = \"/kaggle/input/full-dataset/audio_train\"\n",
    "train_data = \"/kaggle/working/train_full_3f.pkl\"\n",
    "\n",
    "video_test = \"/kaggle/input/full-dataset/clip_test\"\n",
    "text_test = \"/kaggle/input/full-dataset/dialogue_test\"\n",
    "audio_test = \"/kaggle/input/full-dataset/audio_test\"\n",
    "test_data = \"/kaggle/working/test_full_3f.pkl\"\n",
    "\n",
    "# Đọc dữ liệu và lưu vào pickle\n",
    "train_data = read_data(video_train, text_train, audio_train, train_data)\n",
    "test_data = read_data(video_test, text_test, audio_test, test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class pre_processing(Dataset):\n",
    "    def __init__(self, data_pickle, label_folder, transform=None, target_size=(128, 128), num_frames=256, label_mapping=None):\n",
    "        self.data_pickle = data_pickle\n",
    "        self.label_folder = label_folder\n",
    "        self.transform = transform\n",
    "        self.target_size = target_size\n",
    "        self.num_frames = num_frames\n",
    "\n",
    "        # Load dữ liệu từ pickle\n",
    "        self.data = self._load_pickle(data_pickle)\n",
    "\n",
    "        # Load labels từ CSV\n",
    "        self.labels = self._load_labels()\n",
    "\n",
    "        # Lưu danh sách các keys (tên file) để truy cập dữ liệu\n",
    "        self.keys = list(self.data.keys())\n",
    "        \n",
    "        # Default label mapping nếu không có\n",
    "        self.label_mapping = label_mapping if label_mapping else {\n",
    "            \"asks\": 0, \"gives to\": 1, \"talks to\": 2, \"walks with\": 3,\n",
    "            \"watches\": 4, \"yells at\": 5, \"no_interaction\": 6\n",
    "        }\n",
    "\n",
    "    def _load_pickle(self, path):\n",
    "        with open(path, \"rb\") as f:\n",
    "            data = pickle.load(f)\n",
    "        return data\n",
    "\n",
    "    def _load_labels(self):\n",
    "        labels = {}\n",
    "\n",
    "        # Duyệt qua tất cả thư mục con trong thư mục cha\n",
    "        for root, dirs, files in os.walk(self.label_folder):\n",
    "            for file in files:\n",
    "                # Kiểm tra nếu file là .csv\n",
    "                if file.endswith('.csv'):\n",
    "                    label_path = os.path.join(root, file)\n",
    "                    \n",
    "                    # Đọc file csv\n",
    "                    df = pd.read_csv(label_path)\n",
    "                    for _, row in df.iterrows():\n",
    "                        # Lưu nhãn tương ứng với clip\n",
    "                        labels[row['Clip']] = row['Interaction']\n",
    "        \n",
    "        return labels\n",
    "\n",
    "    def _read_video(self, video_path):\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        frames = []\n",
    "\n",
    "        while cap.isOpened():\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            frame = self._resize_frame(frame)  # Resize frame về kích thước cố định\n",
    "            frames.append(frame)\n",
    "        cap.release()\n",
    "\n",
    "        # Lấy mẫu số lượng frame cố định\n",
    "        frames = self._sample_frames(frames)\n",
    "\n",
    "        # Chuyển đổi frames thành tensor\n",
    "        frames_array = np.array(frames, dtype=np.float32)\n",
    "        frames_array /= 255.0  # Chuẩn hóa giá trị về [0, 1]\n",
    "\n",
    "        # Chuyển đổi từ (F, H, W, C) sang (C, F, H, W)\n",
    "        video_tensor = torch.from_numpy(frames_array).permute(3, 0, 1, 2)  # (C, F, H, W)\n",
    "        return video_tensor\n",
    "\n",
    "    def _resize_frame(self, frame):\n",
    "        return cv2.resize(frame, self.target_size, interpolation=cv2.INTER_AREA)\n",
    "\n",
    "    def _sample_frames(self, frames):\n",
    "        num_total_frames = len(frames)\n",
    "    \n",
    "        if num_total_frames >= self.num_frames:\n",
    "            # Phân bổ đều các nhóm frame\n",
    "            indices = np.linspace(0, num_total_frames - 1, self.num_frames, dtype=int)\n",
    "            sampled_frames = [frames[idx] for idx in indices]\n",
    "        else:\n",
    "            # Nếu số frame ít hơn, lấy ngẫu nhiên các frame để đủ số lượng\n",
    "            extra_frames = np.random.choice(num_total_frames, self.num_frames - num_total_frames, replace=True)\n",
    "            sampled_frames = frames + [frames[idx] for idx in extra_frames]\n",
    "        return sampled_frames\n",
    "\n",
    "    def _clean_text(self, text):\n",
    "        text = re.sub(r\"<.*?>\", \"\", text)  # Xóa thẻ HTML\n",
    "        text = re.sub(r\"\\([^\\)]+\\)\", \"\", text)  # Xóa văn bản trong dấu ()\n",
    "        text = re.sub(r\"\\[[^\\]]+\\]\", \"\", text)  # Xóa văn bản trong dấu []\n",
    "        text = re.sub(r\"\\s+\", \" \", text).strip()  # Xóa khoảng trắng dư thừa\n",
    "        return text\n",
    "\n",
    "    def _read_srt(self, srt_path):\n",
    "        subs = pysrt.open(srt_path, encoding='utf-8')\n",
    "        dialogues = []\n",
    "        for sub in subs:\n",
    "            dialogue = self._clean_text(sub.text)\n",
    "            dialogues.append(dialogue)\n",
    "        return dialogues\n",
    "\n",
    "    def pad_texts(self, texts, pad_token=\"<PAD>\"):\n",
    "        max_length = max(len(text) for text in texts)  # Độ dài lớn nhất\n",
    "        padded_texts = []\n",
    "        for text in texts:\n",
    "            # Thêm padding token nếu câu ngắn hơn max_length\n",
    "            padded = text + [pad_token] * (max_length - len(text))\n",
    "            padded_texts.append(padded[:max_length])  # Cắt nếu vượt quá max_length\n",
    "        return padded_texts\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.keys)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        key = self.keys[idx]\n",
    "    \n",
    "        video_path = self.data[key][\"video\"]\n",
    "        text_path = self.data[key][\"text\"]\n",
    "        audio_path = self.data[key][\"audio\"]\n",
    "    \n",
    "        video_tensor = self._read_video(video_path)\n",
    "        \n",
    "        text = self._read_srt(text_path) \n",
    "        text = self.pad_texts([text])[0]\n",
    "    \n",
    "        # Lấy nhãn tương ứng\n",
    "        label = self.labels.get(key, None)\n",
    "    \n",
    "        label_idx = self.label_mapping.get(label, -1)  # Lấy chỉ số nhãn từ label_mapping\n",
    "    \n",
    "        # Áp dụng transform (nếu có)\n",
    "        if self.transform:\n",
    "            video_tensor = self.transform(video_tensor)\n",
    "    \n",
    "        return (video_tensor, label_idx, video_path), (text_path, text), audio_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def custom_collate_fn(batch):\n",
    "    \"\"\"\n",
    "    Hàm collate cho DataLoader để kết hợp các mẫu vào batch.\n",
    "    \"\"\"\n",
    "    # Tách các phần tử từ batch\n",
    "    video_tensors = []\n",
    "    labels = []\n",
    "    video_paths = []\n",
    "    text_paths = []\n",
    "    texts = []\n",
    "    audio_paths = []\n",
    "    \n",
    "    for item in batch:\n",
    "        (video_tensor, label, video_path), (text_path, text), audio_path = item\n",
    "        video_tensors.append(video_tensor)\n",
    "        labels.append(label)\n",
    "        video_paths.append(video_path)\n",
    "        text_paths.append(text_path)\n",
    "        texts.append(text)\n",
    "        audio_paths.append(audio_path)\n",
    "\n",
    "    # Kết hợp video tensor thành batch\n",
    "    video_tensors = torch.stack(video_tensors)  # (batch_size, C, F, H, W)\n",
    "\n",
    "    max_text_len = max(len(t) for t in texts)\n",
    "    padded_texts = [\n",
    "        t + [\"<PAD>\"] * (max_text_len - len(t)) for t in texts\n",
    "    ]\n",
    "\n",
    "    # Trả về dictionary\n",
    "    return {\n",
    "        \"videos\": video_tensors,\n",
    "        \"labels\": labels,\n",
    "        \"video_paths\": video_paths,\n",
    "        \"text_paths\": text_paths,\n",
    "        \"texts\": padded_texts,\n",
    "        \"audio_paths\": audio_paths,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Tạo dataset từ pickle\n",
    "train_dataset = pre_processing(\n",
    "    data_pickle='/kaggle/input/checkpoint/full_v_t_a/train_full_data.pkl',\n",
    "    label_folder='/kaggle/input/full-dataset/train_labels',\n",
    "    transform=None\n",
    ")\n",
    "\n",
    "test_dataset = pre_processing(\n",
    "    data_pickle='/kaggle/input/checkpoint/full_v_t_a/test_full_data.pkl',\n",
    "    label_folder='/kaggle/input/full-dataset/test_labels',\n",
    "    transform=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=16,\n",
    "    shuffle=True,\n",
    "    num_workers=2,\n",
    "    pin_memory=True,\n",
    "    collate_fn=custom_collate_fn\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=16,\n",
    "    shuffle=False,\n",
    "    num_workers=2,\n",
    "    pin_memory=True,\n",
    "    collate_fn=custom_collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "dataset_size = len(train_loader)\n",
    "print(dataset_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "\n",
    "def count_labels(label_folder):\n",
    "    labels = []\n",
    "\n",
    "    for root, dirs, files in os.walk(label_folder):\n",
    "        for file in files:\n",
    "            # Check if the file is a .csv\n",
    "            if file.endswith('.csv'):\n",
    "                label_path = os.path.join(root, file)\n",
    "                \n",
    "                # Read the CSV file\n",
    "                df = pd.read_csv(label_path)\n",
    "                labels.extend(df['Interaction'].tolist())\n",
    "    \n",
    "    return labels\n",
    "\n",
    "# Specify the label folder\n",
    "label_folder = '/kaggle/input/full-dataset/train_labels'\n",
    "all_labels = count_labels(label_folder)\n",
    "label_counts = Counter(all_labels)\n",
    "\n",
    "# Define the fixed order of the classes (e.g., sorted alphabetically)\n",
    "fixed_order = [\"asks\", \"gives to\", \"talks to\", \"walks with\", \"watches\", \"yells at\", \"no_interaction\"]\n",
    "\n",
    "# Sort the label_counts keys based on the fixed order\n",
    "sorted_labels = sorted(label_counts.keys(), key=lambda x: fixed_order.index(x) if x in fixed_order else len(fixed_order))\n",
    "\n",
    "# Sort the counts based on the sorted labels\n",
    "sorted_counts = [label_counts[label] for label in sorted_labels]\n",
    "\n",
    "# Plot the bar chart\n",
    "plt.figure(figsize=(8, 6))\n",
    "bars = plt.bar(sorted_labels, sorted_counts, color='darkcyan')\n",
    "plt.xlabel('Classes')\n",
    "plt.ylabel('Number of Samples')\n",
    "plt.title('Distribution of Samples in TrainSet')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# Add label on top of each bar\n",
    "for bar in bars:\n",
    "    yval = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, yval + 1, int(yval), ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ViViT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# helpers\n",
    "def exists(val):\n",
    "    return val is not None\n",
    "\n",
    "def pair(t):\n",
    "    return t if isinstance(t, tuple) else (t, t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    \n",
    "    def __init__(self, dim, hidden_dim, dropout = 0.3):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.LayerNorm(dim),\n",
    "            nn.Linear(dim, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim, heads = 8, dim_head = 64, dropout = 0.3):\n",
    "        super().__init__()\n",
    "        inner_dim = dim_head *  heads\n",
    "        project_out = not (heads == 1 and dim_head == dim)\n",
    "\n",
    "        self.heads = heads\n",
    "        self.scale = dim_head ** -0.5\n",
    "\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "        self.attend = nn.Softmax(dim = -1)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.to_qkv = nn.Linear(dim, inner_dim * 3, bias = False)\n",
    "\n",
    "        self.to_out = nn.Sequential(\n",
    "            nn.Linear(inner_dim, dim),\n",
    "            nn.Dropout(dropout)\n",
    "        ) if project_out else nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.norm(x)\n",
    "        qkv = self.to_qkv(x).chunk(3, dim = -1)\n",
    "        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h = self.heads), qkv)\n",
    "\n",
    "        dots = torch.matmul(q, k.transpose(-1, -2)) * self.scale\n",
    "\n",
    "        attn = self.attend(dots)\n",
    "        attn = self.dropout(attn)\n",
    "\n",
    "        out = torch.matmul(attn, v)\n",
    "        out = rearrange(out, 'b h n d -> b n (h d)')\n",
    "        return self.to_out(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, dim, depth, heads, dim_head, mlp_dim, dropout = 0.3):\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "        self.layers = nn.ModuleList([])\n",
    "        for _ in range(depth):\n",
    "            self.layers.append(nn.ModuleList([\n",
    "                Attention(dim, heads = heads, dim_head = dim_head, dropout = dropout),\n",
    "                FeedForward(dim, mlp_dim, dropout = dropout)]))\n",
    "    def forward(self, x):\n",
    "        for attn, ff in self.layers:\n",
    "            x = attn(x) + x\n",
    "            x = ff(x) + x\n",
    "        return self.norm(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class FactorizedTransformer(nn.Module):\n",
    "    def __init__(self, dim, depth, heads, dim_head, mlp_dim, dropout = 0.2):\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "        self.layers = nn.ModuleList([])\n",
    "        for _ in range(depth):\n",
    "            self.layers.append(nn.ModuleList([\n",
    "                Attention(dim, heads = heads, dim_head = dim_head, dropout = dropout),\n",
    "                Attention(dim, heads = heads, dim_head = dim_head, dropout = dropout),\n",
    "                FeedForward(dim, mlp_dim, dropout = dropout)\n",
    "            ]))\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, f, n, _ = x.shape\n",
    "        for spatial_attn, temporal_attn, ff in self.layers:\n",
    "            x = rearrange(x, 'b f n d -> (b f) n d')\n",
    "            x = spatial_attn(x) + x\n",
    "            x = rearrange(x, '(b f) n d -> (b n) f d', b=b, f=f)\n",
    "            x = temporal_attn(x) + x\n",
    "            x = ff(x) + x\n",
    "            x = rearrange(x, '(b n) f d -> b f n d', b=b, n=n)\n",
    "\n",
    "        return self.norm(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class TubeletEmbedding(nn.Module):\n",
    "    def __init__(self, channels, tubelet_size, patch_size, dim):\n",
    "        super().__init__()\n",
    "        self.tubelet_size = tubelet_size\n",
    "        self.patch_size = patch_size\n",
    "        patch_dim = channels * tubelet_size * patch_size ** 2\n",
    "        self.to_patch_embedding = nn.Sequential(\n",
    "            # Điều chỉnh Rearrange để có đủ số chiều\n",
    "            Rearrange('b c (f tf) (h ph) (w pw) -> b f (h w) (tf ph pw c)',\n",
    "                      tf=tubelet_size, ph=patch_size, pw=patch_size),\n",
    "            nn.LayerNorm(patch_dim),\n",
    "            nn.Linear(patch_dim, dim),\n",
    "            nn.LayerNorm(dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, video):\n",
    "        return self.to_patch_embedding(video)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class Video_ViT(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        *,\n",
    "        image_size,\n",
    "        image_patch_size,\n",
    "        frames,\n",
    "        frame_patch_size,\n",
    "        num_classes,\n",
    "        dim,\n",
    "        spatial_depth,\n",
    "        temporal_depth,\n",
    "        heads,\n",
    "        mlp_dim,\n",
    "        pool = 'cls',\n",
    "        channels = 3,\n",
    "        dim_head = 64,\n",
    "        dropout = 0.3,\n",
    "        emb_dropout = 0.3,\n",
    "        variant = 'factorized_self_attention',):\n",
    "        \n",
    "        super().__init__()\n",
    "        image_height, image_width = pair(image_size)\n",
    "        patch_height, patch_width = pair(image_patch_size)\n",
    "\n",
    "        assert image_height % patch_height == 0 and image_width % patch_width == 0, 'Image dimensions must be divisible by the patch size.'\n",
    "        assert frames % frame_patch_size == 0, 'Frames must be divisible by frame patch size'\n",
    "        assert variant in ('factorized_encoder', 'factorized_self_attention'), f'variant = {variant} is not implemented'\n",
    "\n",
    "        num_image_patches = (image_height // patch_height) * (image_width // patch_width)\n",
    "        num_frame_patches = (frames // frame_patch_size)\n",
    "\n",
    "        #patch_dim = channels * patch_height * patch_width * frame_patch_size\n",
    "\n",
    "        assert pool in {'cls', 'mean'}, 'pool type must be either cls (cls token) or mean (mean pooling)'\n",
    "\n",
    "        self.global_average_pool = pool == 'mean'\n",
    "        \n",
    "        #Tubelet or Uniform sampling\n",
    "        \n",
    "        self.to_patch_embedding = TubeletEmbedding(\n",
    "            channels=channels,\n",
    "            tubelet_size=frame_patch_size,\n",
    "            patch_size=patch_height,\n",
    "            dim=dim)\n",
    "\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1, num_frame_patches, num_image_patches, dim))\n",
    "        self.dropout = nn.Dropout(emb_dropout)\n",
    "\n",
    "        self.spatial_cls_token = nn.Parameter(torch.randn(1, 1, dim)) if not self.global_average_pool else None\n",
    "\n",
    "        if variant == 'factorized_encoder':\n",
    "            self.temporal_cls_token = nn.Parameter(torch.randn(1, 1, dim)) if not self.global_average_pool else None\n",
    "            self.spatial_transformer = Transformer(dim, spatial_depth, heads, dim_head, mlp_dim, dropout)\n",
    "            self.temporal_transformer = Transformer(dim, temporal_depth, heads, dim_head, mlp_dim, dropout)\n",
    "        elif variant == 'factorized_self_attention':\n",
    "            assert spatial_depth == temporal_depth, 'Spatial and temporal depth must be the same for factorized self-attention'\n",
    "            self.factorized_transformer = FactorizedTransformer(dim, spatial_depth, heads, dim_head, mlp_dim, dropout)\n",
    "\n",
    "        self.pool = pool\n",
    "        self.to_latent = nn.Identity()\n",
    "\n",
    "        #self.mlp_head = nn.Linear(dim, num_classes)\n",
    "        self.variant = variant\n",
    "             \n",
    "    def extract_features(self, video):\n",
    "        x = self.to_patch_embedding(video)\n",
    "        b, f, n, _ = x.shape\n",
    "        x = x + self.pos_embedding[:, :f, :n]\n",
    "\n",
    "        if exists(self.spatial_cls_token):\n",
    "            spatial_cls_tokens = repeat(self.spatial_cls_token, '1 1 d -> b f 1 d', b=b, f=f)\n",
    "            x = torch.cat((spatial_cls_tokens, x), dim=2)\n",
    "\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        if self.variant == 'factorized_encoder':\n",
    "            x = rearrange(x, 'b f n d -> (b f) n d')\n",
    "            x = self.spatial_transformer(x)\n",
    "            x = rearrange(x, '(b f) n d -> b f n d', b=b)\n",
    "            x = x[:, :, 0] if not self.global_average_pool else reduce(x, 'b f n d -> b f d', 'mean')\n",
    "            if exists(self.temporal_cls_token):\n",
    "                temporal_cls_tokens = repeat(self.temporal_cls_token, '1 1 d-> b 1 d', b=b)\n",
    "                x = torch.cat((temporal_cls_tokens, x), dim=1)\n",
    "            x = self.temporal_transformer(x)\n",
    "            x = x[:, 0] if not self.global_average_pool else reduce(x, 'b f d -> b d', 'mean')\n",
    "\n",
    "        elif self.variant == 'factorized_self_attention':\n",
    "            x = self.factorized_transformer(x)\n",
    "            x = x[:, 0, 0] if not self.global_average_pool else reduce(x, 'b f n d -> b d', 'mean')\n",
    "\n",
    "        return x\n",
    "\n",
    "    def forward(self, video):\n",
    "        x = self.extract_features(video)\n",
    "        return x  # Trả về đặc trưng video mà không qua lớp phân loại"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visual features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jQrnCjs-dM5o",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "ViViT_model = Video_ViT(\n",
    "    image_size = 128,          # image size\n",
    "    frames = 256,               # number of frames\n",
    "    image_patch_size = 16,     # image patch size\n",
    "    frame_patch_size = 2,      # frame patch size\n",
    "    num_classes = 7,\n",
    "    dim = 1024,\n",
    "    spatial_depth = 6,         # depth of the spatial transformer\n",
    "    temporal_depth = 6,        # depth of the temporal transformer\n",
    "    heads = 8,\n",
    "    mlp_dim = 2048,\n",
    "    variant = 'factorized_self_attention', # or 'factorized_encoder'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "ViViT_model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bert:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\", clean_up_tokenization_spaces = True)  # Load tokenizer của BERT\n",
    "bert_model = AutoModel.from_pretrained(\"bert-base-uncased\")  # Load mô hình BERT\n",
    "\n",
    "bert_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def process_texts(texts, tokenizer, device, max_length=512):\n",
    "    # Chuẩn hóa văn bản: loại bỏ ký tự đặc biệt\n",
    "    texts = [re.sub(r'[^\\w\\s.,!?-]', '', str(text)) for text in texts]\n",
    "    \n",
    "    # Token hóa văn bản\n",
    "    encoded = tokenizer(texts, padding=True, truncation=True, return_tensors=\"pt\", max_length=max_length)\n",
    "    input_ids = encoded[\"input_ids\"].to(device)\n",
    "    attention_mask = encoded[\"attention_mask\"].to(device)\n",
    "    \n",
    "    return input_ids, attention_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Audio feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from transformers import AutoFeatureExtractor, ASTForAudioClassification\n",
    "\n",
    "feature_extractor = AutoFeatureExtractor.from_pretrained(\"MIT/ast-finetuned-audioset-10-10-0.4593\")\n",
    "AST_model = ASTForAudioClassification.from_pretrained(\"MIT/ast-finetuned-audioset-10-10-0.4593\")\n",
    "\n",
    "AST_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def extract_audio_features(audio_paths):\n",
    "    features = []\n",
    "    for audio_path in audio_paths:\n",
    "        waveform, sample_rate = torchaudio.load(audio_path)\n",
    "        waveform = waveform.mean(dim=0, keepdim=True)  # Convert stereo to mono nếu cần\n",
    "\n",
    "        # Resample về 16kHz nếu không đúng\n",
    "        if sample_rate != 16000:\n",
    "            resample_transform = torchaudio.transforms.Resample(orig_freq=sample_rate, new_freq=16000)\n",
    "            waveform = resample_transform(waveform)\n",
    "\n",
    "        # Chuyển đổi waveform thành input cho AST model\n",
    "        inputs = feature_extractor(waveform.numpy(), sampling_rate=16000, return_tensors=\"pt\", padding=True)\n",
    "        \n",
    "        # Trích xuất đặc trưng mà không cần thực hiện phân loại\n",
    "        with torch.no_grad():\n",
    "            outputs = AST_model.audio_spectrogram_transformer(**inputs)\n",
    "            feature = outputs.last_hidden_state\n",
    "\n",
    "        pooled_features = torch.mean(feature, dim=1)\n",
    "        features.append(pooled_features.squeeze(0)) \n",
    "    return torch.stack(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def extract_features(data_loader, visual_model, text_model, tokenizer, device, output_path):\n",
    "    visual_features_list = []\n",
    "    text_features_list = []\n",
    "    audio_features_list = []\n",
    "    \n",
    "    labels_list = []\n",
    "    \n",
    "    video_paths_list = []\n",
    "    text_paths_list = [] \n",
    "    audio_paths_list = [] \n",
    "\n",
    "    visual_model.to(device)\n",
    "    text_model.to(device)\n",
    "    \n",
    "    for i, batch in enumerate(data_loader):\n",
    "        videos = batch['videos'].float().to(device)\n",
    "        texts = batch['texts']\n",
    "        labels = batch['labels']\n",
    "        \n",
    "        video_paths = batch['video_paths']\n",
    "        text_paths = batch['text_paths']\n",
    "        audio_paths = batch['audio_paths']\n",
    "\n",
    "\n",
    "        with torch.no_grad():\n",
    "            visual_features = visual_model.extract_features(videos)\n",
    "\n",
    "        \n",
    "        input_ids, attention_mask = process_texts(texts, tokenizer, device)\n",
    "        with torch.no_grad():\n",
    "            outputs = text_model(input_ids, attention_mask=attention_mask)\n",
    "            cls_embeddings = outputs.last_hidden_state[:, 0, :]  # Đặc trưng từ CLS token\n",
    "\n",
    "        \n",
    "        audio_embeddings = extract_audio_features(audio_paths)\n",
    "    \n",
    "        # Lưu các đặc trưng và labels vào danh sách\n",
    "        visual_features_list.extend(visual_features.tolist())\n",
    "        text_features_list.extend(cls_embeddings.tolist())\n",
    "        audio_features_list.extend(audio_embeddings.tolist())\n",
    "        labels_list.extend(labels)\n",
    "        \n",
    "        video_paths_list.extend(video_paths)\n",
    "        text_paths_list.extend(text_paths)\n",
    "        audio_paths_list.extend(audio_paths)\n",
    "\n",
    "        # In ra thông báo mỗi 50 batch\n",
    "        if (i + 1) % 50 == 0:\n",
    "            print(f\"Batch {i + 1}/{len(data_loader)} completed.\")\n",
    "\n",
    "    features_data = {\n",
    "        'video_paths': video_paths_list,\n",
    "        'visual_features': visual_features_list,\n",
    "        'text_paths': text_paths_list,\n",
    "        'text_features': text_features_list,\n",
    "        'audio_paths': audio_paths_list,\n",
    "        'audio_features': audio_features_list,\n",
    "        'labels': labels_list,\n",
    "    }\n",
    "\n",
    "    with open(output_path, \"wb\") as f:\n",
    "        pickle.dump(features_data, f)\n",
    "    \n",
    "    print(f\"Features saved to {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_feats = '/kaggle/working/train_full_v_t_a.pkl'\n",
    "extract_features(train_loader, ViViT_model, bert_model, tokenizer, device, train_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "test_feats = '/kaggle/working/test_full_v_t_a.pkl'\n",
    "extract_features(test_loader, ViViT_model, bert_model, tokenizer, device, test_feats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-28T14:57:56.862321Z",
     "iopub.status.busy": "2025-03-28T14:57:56.861924Z",
     "iopub.status.idle": "2025-03-28T14:57:56.867593Z",
     "shell.execute_reply": "2025-03-28T14:57:56.866558Z",
     "shell.execute_reply.started": "2025-03-28T14:57:56.862296Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def load_features(input_path):\n",
    "    with open(input_path, \"rb\") as f:\n",
    "        features_data = pickle.load(f)\n",
    "\n",
    "    video_paths = features_data['video_paths']\n",
    "    visual_features = torch.tensor(features_data['visual_features'])\n",
    "    \n",
    "    text_paths = features_data['text_paths']\n",
    "    text_features = torch.tensor(features_data['text_features'])\n",
    "    \n",
    "    audio_paths = features_data['audio_paths']\n",
    "    audio_features = torch.tensor(features_data['audio_features'])\n",
    "    \n",
    "    labels = torch.tensor(features_data['labels']).to(device)\n",
    "\n",
    "    return video_paths, visual_features, text_paths, text_features, audio_paths, audio_features, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-28T14:58:01.652898Z",
     "iopub.status.busy": "2025-03-28T14:58:01.652527Z",
     "iopub.status.idle": "2025-03-28T14:58:01.660386Z",
     "shell.execute_reply": "2025-03-28T14:58:01.659305Z",
     "shell.execute_reply.started": "2025-03-28T14:58:01.652868Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def group_features(video_paths, visual_features, text_paths, text_features, audio_paths, audio_features, labels):\n",
    "    scene_feats = {}\n",
    "\n",
    "    for i, video_path in enumerate(video_paths):\n",
    "        # Extract scene name from video path\n",
    "        scene_name = \"-\".join(os.path.basename(video_path).split(\"-\")[:2])\n",
    "\n",
    "        if scene_name not in scene_feats:\n",
    "            scene_feats[scene_name] = {\n",
    "                'visual_features': [],\n",
    "                'text_features': [],\n",
    "                'audio_features': [],\n",
    "                \n",
    "                'labels': [],\n",
    "                'video_paths': [],\n",
    "                'text_paths': [],\n",
    "                'audio_paths': [],\n",
    "            }\n",
    "\n",
    "        scene_feats[scene_name]['visual_features'].append(visual_features[i].tolist())\n",
    "        scene_feats[scene_name]['text_features'].append(text_features[i].tolist())\n",
    "        scene_feats[scene_name]['audio_features'].append(audio_features[i].tolist())\n",
    "        \n",
    "        scene_feats[scene_name]['labels'].append(labels[i].tolist())\n",
    "        \n",
    "        scene_feats[scene_name]['video_paths'].append(video_paths[i])\n",
    "        scene_feats[scene_name]['text_paths'].append(text_paths[i])\n",
    "        scene_feats[scene_name]['audio_paths'].append(audio_paths[i])\n",
    "\n",
    "    # Convert lists to tensors\n",
    "    for scene_name in scene_feats:\n",
    "        scene_feats[scene_name]['visual_features'] = torch.tensor(scene_feats[scene_name]['visual_features'])\n",
    "        scene_feats[scene_name]['text_features'] = torch.tensor(scene_feats[scene_name]['text_features'])\n",
    "        scene_feats[scene_name]['audio_features'] = torch.tensor(scene_feats[scene_name]['audio_features'])\n",
    "\n",
    "        scene_feats[scene_name]['labels'] = torch.tensor(scene_feats[scene_name]['labels'])\n",
    "\n",
    "    return scene_feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-28T14:58:11.899843Z",
     "iopub.status.busy": "2025-03-28T14:58:11.899466Z",
     "iopub.status.idle": "2025-03-28T14:58:20.228712Z",
     "shell.execute_reply": "2025-03-28T14:58:20.227679Z",
     "shell.execute_reply.started": "2025-03-28T14:58:11.899811Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grouped features saved\n"
     ]
    }
   ],
   "source": [
    "features_path = \"/kaggle/input/checkpoint/full_v_t_a/train_full_v_t_a.pkl\"\n",
    "video_paths, visual_features, text_paths, text_features, audio_paths, audio_features, labels = load_features(features_path)\n",
    "scene_feats = group_features(video_paths, visual_features, text_paths, text_features, audio_paths, audio_features, labels)\n",
    "\n",
    "with open(\"/kaggle/working/grouped_features.pkl\", \"wb\") as f:\n",
    "    pickle.dump(scene_feats, f)\n",
    "print(f\"Grouped features saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-28T14:58:23.813588Z",
     "iopub.status.busy": "2025-03-28T14:58:23.813172Z",
     "iopub.status.idle": "2025-03-28T14:58:23.825707Z",
     "shell.execute_reply": "2025-03-28T14:58:23.824739Z",
     "shell.execute_reply.started": "2025-03-28T14:58:23.813553Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "for scene_name, scene_data in scene_feats.items():\n",
    "    scene_data['labels'] = F.one_hot(scene_data['labels'], num_classes=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-28T14:58:30.926002Z",
     "iopub.status.busy": "2025-03-28T14:58:30.925637Z",
     "iopub.status.idle": "2025-03-28T14:58:30.933435Z",
     "shell.execute_reply": "2025-03-28T14:58:30.932533Z",
     "shell.execute_reply.started": "2025-03-28T14:58:30.925959Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def padding(features, max_length, pad_value=0):\n",
    "    \"\"\"\n",
    "    Hàm padding cho tensor, hỗ trợ cả 1D và 2D tensor.\n",
    "    \"\"\"\n",
    "    if len(features.shape) == 1:\n",
    "        # Padding cho tensor 1D\n",
    "        padded_features = torch.nn.functional.pad(\n",
    "            features, (0, max_length - features.size(0)), value=pad_value\n",
    "        )\n",
    "    elif len(features.shape) == 2:\n",
    "        # Padding cho tensor 2D\n",
    "        padded_features = torch.nn.functional.pad(\n",
    "            features, (0, 0, 0, max_length - features.size(0)), value=pad_value\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported tensor shape: {features.shape}\")\n",
    "    return padded_features\n",
    "\n",
    "def pad_scene_level(scene_data):\n",
    "    \"\"\"\n",
    "    Hàm padding tất cả các scene để đảm bảo có cùng độ dài sequence.\n",
    "    \"\"\"\n",
    "    scene_names = list(scene_data.keys())\n",
    "\n",
    "    # Tính max_length từ visual_features\n",
    "    max_length = max(scene_data[scene_name]['visual_features'].size(0) for scene_name in scene_names)\n",
    "\n",
    "    for scene_name in scene_names:\n",
    "        try:\n",
    "            visual_features = scene_data[scene_name]['visual_features']\n",
    "            text_features = scene_data[scene_name]['text_features']\n",
    "            audio_features = scene_data[scene_name]['audio_features']\n",
    "            labels = scene_data[scene_name]['labels']\n",
    "\n",
    "            # Padding cho từng loại dữ liệu\n",
    "            scene_data[scene_name]['visual_features'] = padding(visual_features, max_length, pad_value=0) \n",
    "            scene_data[scene_name]['text_features'] = padding(text_features, max_length, pad_value=0)\n",
    "            scene_data[scene_name]['audio_features'] = padding(audio_features, max_length, pad_value=0)      \n",
    "\n",
    "            scene_data[scene_name]['labels'] = padding(labels, max_length, pad_value=0)                 \n",
    "        except KeyError as e:\n",
    "            raise KeyError(f\"Missing key in scene data: {e}\")\n",
    "    return scene_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-28T14:58:35.160439Z",
     "iopub.status.busy": "2025-03-28T14:58:35.160110Z",
     "iopub.status.idle": "2025-03-28T14:58:35.249839Z",
     "shell.execute_reply": "2025-03-28T14:58:35.249126Z",
     "shell.execute_reply.started": "2025-03-28T14:58:35.160416Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "scene_feats_padded = pad_scene_level(scene_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-28T14:58:37.898060Z",
     "iopub.status.busy": "2025-03-28T14:58:37.897723Z",
     "iopub.status.idle": "2025-03-28T14:58:38.101653Z",
     "shell.execute_reply": "2025-03-28T14:58:38.100652Z",
     "shell.execute_reply.started": "2025-03-28T14:58:37.898036Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "visual_features = []\n",
    "text_features = []\n",
    "audio_features = []\n",
    "\n",
    "labels = []\n",
    "\n",
    "for scene_name, scene in scene_feats_padded.items():\n",
    "    visual_features.append(scene['visual_features'])\n",
    "    text_features.append(scene['text_features'])\n",
    "    audio_features.append(scene['audio_features'])\n",
    "    \n",
    "    labels.append(scene['labels'])\n",
    "\n",
    "# Chuyển các list thành tensor\n",
    "visual_features = torch.stack(visual_features)\n",
    "text_features = torch.stack(text_features)\n",
    "audio_features = torch.stack(audio_features)\n",
    "labels = torch.stack(labels)\n",
    "\n",
    "train_feats = TensorDataset(visual_features, text_features, audio_features, labels)\n",
    "\n",
    "train_size = int(0.78 * len(train_feats))\n",
    "val_size = len(train_feats) - train_size\n",
    "train, val = random_split(train_feats, [train_size, val_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Kiểm tra kết quả padding\n",
    "'''for scene_name, scene in scene_feats_padded.items():\n",
    "    print(f\"Scene: {scene_name}\")\n",
    "    print(f\"Visual features:\\n{scene['visual_features'].shape}\")\n",
    "    #print(f\"  Text features shape: {scene['text_features'].shape}\")\n",
    "    #print(f\"  Audio features shape: {scene['audio_features'].shape}\")\n",
    "    print(f\"  Labels shape: {scene['labels'].shape}\")\n",
    "    break'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-28T14:58:41.245239Z",
     "iopub.status.busy": "2025-03-28T14:58:41.244840Z",
     "iopub.status.idle": "2025-03-28T14:58:41.291183Z",
     "shell.execute_reply": "2025-03-28T14:58:41.290241Z",
     "shell.execute_reply.started": "2025-03-28T14:58:41.245207Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set: 397 scenes\n",
      "Validation set: 112 scenes\n"
     ]
    }
   ],
   "source": [
    "train_feats_loader = DataLoader(train, batch_size=16, shuffle=True)\n",
    "val_feats_loader = DataLoader(val, batch_size=16, shuffle=True)\n",
    "\n",
    "print(f\"Train set: {len(train_feats_loader.dataset)} scenes\")\n",
    "print(f\"Validation set: {len(val_feats_loader.dataset)} scenes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-28T14:58:46.910878Z",
     "iopub.status.busy": "2025-03-28T14:58:46.910581Z",
     "iopub.status.idle": "2025-03-28T14:58:46.916309Z",
     "shell.execute_reply": "2025-03-28T14:58:46.915281Z",
     "shell.execute_reply.started": "2025-03-28T14:58:46.910855Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class CrossAttention(nn.Module):\n",
    "    def __init__(self, dim, num_heads=8, dropout=0.4):\n",
    "        super(CrossAttention, self).__init__()\n",
    "        self.multihead_attn = nn.MultiheadAttention(embed_dim=dim, num_heads=num_heads, dropout=dropout, batch_first=True)\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, context, attn_mask=None, key_padding_mask=None):\n",
    "        attn_output, _ = self.multihead_attn(\n",
    "            x, context, context, \n",
    "            attn_mask=attn_mask, \n",
    "            key_padding_mask=key_padding_mask\n",
    "        )\n",
    "        attn_output = self.dropout(attn_output)\n",
    "        return self.norm(attn_output + x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-28T14:58:55.837247Z",
     "iopub.status.busy": "2025-03-28T14:58:55.836708Z",
     "iopub.status.idle": "2025-03-28T14:58:55.848831Z",
     "shell.execute_reply": "2025-03-28T14:58:55.847766Z",
     "shell.execute_reply.started": "2025-03-28T14:58:55.837201Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class MultimodalLSTM(nn.Module):\n",
    "    def __init__(self, hidden_dim=512, num_labels=7, dropout_rate=0.3):\n",
    "        super(MultimodalLSTM, self).__init__()\n",
    "\n",
    "        # LSTM layers (Bidirectional → output size = hidden_dim * 2)\n",
    "        self.visual_lstm = nn.LSTM(input_size=1024, hidden_size=hidden_dim, num_layers=2, batch_first=True, bidirectional=True)\n",
    "        self.text_lstm = nn.LSTM(input_size=768, hidden_size=hidden_dim, num_layers=2, batch_first=True, bidirectional=True)\n",
    "        self.audio_lstm = nn.LSTM(input_size=768, hidden_size=hidden_dim, num_layers=2, batch_first=True, bidirectional=True)\n",
    "\n",
    "        # Cross-Attention (adjusted to hidden_dim * 2 due to bidirectional LSTM)\n",
    "        self.cross_attn_vt = CrossAttention(hidden_dim * 2)  \n",
    "        self.cross_attn_va = CrossAttention(hidden_dim * 2)  \n",
    "        self.cross_attn_ta = CrossAttention(hidden_dim * 2) \n",
    "\n",
    "        # Fusion MLP\n",
    "        self.fusion_fc1 = nn.Sequential(\n",
    "            nn.Linear(hidden_dim * 6, 1024),  # hidden_dim * 2 per modality → 6 modalities\n",
    "            nn.ReLU(),\n",
    "            nn.LayerNorm(1024),\n",
    "            nn.Dropout(dropout_rate)\n",
    "        )\n",
    "\n",
    "        self.fusion_fc2 = nn.Sequential(\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.LayerNorm(512),\n",
    "            nn.Dropout(dropout_rate)\n",
    "        )\n",
    "\n",
    "        self.fusion_fc3 = nn.Sequential(\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.LayerNorm(256),\n",
    "            nn.Dropout(dropout_rate)\n",
    "        )\n",
    "\n",
    "        self.output_layer = nn.Linear(256, num_labels)\n",
    "\n",
    "    def forward(self, visual_input, text_input, audio_input):\n",
    "\n",
    "        # LSTM Encoding\n",
    "        visual_feat, _ = self.visual_lstm(visual_input)  \n",
    "        text_feat, _ = self.text_lstm(text_input)    \n",
    "        audio_feat, _ = self.audio_lstm(audio_input)\n",
    "\n",
    "        # Cross-Attention (Parallel Processing)\n",
    "        visual_feat = self.cross_attn_vt(visual_feat, text_feat)\n",
    "        visual_feat = self.cross_attn_va(visual_feat, audio_feat)\n",
    "\n",
    "        text_feat = self.cross_attn_ta(text_feat, audio_feat)\n",
    "        text_feat = self.cross_attn_vt(text_feat, visual_feat)\n",
    "\n",
    "        audio_feat = self.cross_attn_va(audio_feat, visual_feat)\n",
    "        audio_feat = self.cross_attn_ta(audio_feat, text_feat)\n",
    "\n",
    "        # Global average pooling (reduce temporal dimension)\n",
    "        visual_feat = torch.mean(visual_feat, dim=1)  \n",
    "        text_feat = torch.mean(text_feat, dim=1)  \n",
    "        audio_feat = torch.mean(audio_feat, dim=1)  \n",
    "\n",
    "        # Concatenation of all modalities\n",
    "        fusion_out = torch.cat((visual_feat, text_feat, audio_feat), dim=-1)  # (batch_size, hidden_dim * 6)\n",
    "\n",
    "        # MLP Fusion\n",
    "        fusion_out = self.fusion_fc1(fusion_out)\n",
    "        fusion_out = self.fusion_fc2(fusion_out)\n",
    "        fusion_out = self.fusion_fc3(fusion_out)\n",
    "\n",
    "        # Output layer\n",
    "        output = self.output_layer(fusion_out)  # (batch_size, num_labels)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## cross-attention base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "'''class CrossAttention(nn.Module):\n",
    "    def __init__(self, dim, num_heads=8, dropout=0.4):\n",
    "        super(CrossAttention, self).__init__()\n",
    "        self.multihead_attn = nn.MultiheadAttention(embed_dim=dim, num_heads=num_heads, dropout=dropout, batch_first=True)\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, context, attn_mask=None, key_padding_mask=None):\n",
    "        attn_output, _ = self.multihead_attn(\n",
    "            x, context, context, \n",
    "            attn_mask=attn_mask, \n",
    "            key_padding_mask=key_padding_mask\n",
    "        )\n",
    "        attn_output = self.dropout(attn_output)\n",
    "        return self.norm(attn_output + x)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "'''class MultimodalLSTM(nn.Module):\n",
    "    def __init__(self, hidden_dim=512, num_labels=7, dropout_rate=0.3):\n",
    "        super(MultimodalLSTM, self).__init__()\n",
    "\n",
    "        # LSTM layers (Bidirectional → output size = hidden_dim * 2)\n",
    "        self.visual_lstm = nn.LSTM(input_size=1024, hidden_size=hidden_dim, num_layers=2, batch_first=True, bidirectional=True)\n",
    "        self.text_lstm = nn.LSTM(input_size=768, hidden_size=hidden_dim, num_layers=2, batch_first=True, bidirectional=True)\n",
    "        self.audio_lstm = nn.LSTM(input_size=768, hidden_size=hidden_dim, num_layers=2, batch_first=True, bidirectional=True)\n",
    "\n",
    "        # Cross-Attention\n",
    "        self.cross_attn_vt = CrossAttention(hidden_dim * 2)\n",
    "        self.cross_attn_va = CrossAttention(hidden_dim * 2)\n",
    "        self.cross_attn_ta = CrossAttention(hidden_dim * 2)\n",
    "\n",
    "        # Fusion MLP\n",
    "        self.fusion_fc1 = nn.Sequential(\n",
    "            nn.Linear(hidden_dim * 6, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.LayerNorm(1024),\n",
    "            nn.Dropout(dropout_rate)\n",
    "        )\n",
    "\n",
    "        self.fusion_fc2 = nn.Sequential(\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.LayerNorm(512),\n",
    "            nn.Dropout(dropout_rate)\n",
    "        )\n",
    "\n",
    "        self.fusion_fc3 = nn.Sequential(\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.LayerNorm(256),\n",
    "            nn.Dropout(dropout_rate)\n",
    "        )\n",
    "\n",
    "        self.output_layer = nn.Linear(256, num_labels)\n",
    "\n",
    "    def forward(self, visual_input, text_input, audio_input):\n",
    "        # Normalize inputs\n",
    "        text_input = (text_input - text_input.mean(dim=1, keepdim=True)) / (text_input.std(dim=1, keepdim=True) + 1e-6)\n",
    "        audio_input = (audio_input - audio_input.mean(dim=1, keepdim=True)) / (audio_input.std(dim=1, keepdim=True) + 1e-6)\n",
    "\n",
    "        # LSTM Encoding\n",
    "        visual_feat, _ = self.visual_lstm(visual_input)\n",
    "        text_feat, _ = self.text_lstm(text_input)\n",
    "        audio_feat, _ = self.audio_lstm(audio_input)\n",
    "\n",
    "        # Cross-Attention\n",
    "        visual_feat = self.cross_attn_vt(visual_feat, text_feat)\n",
    "        visual_feat = self.cross_attn_va(visual_feat, audio_feat)\n",
    "        text_feat = self.cross_attn_ta(text_feat, audio_feat)\n",
    "\n",
    "        # Global average pooling\n",
    "        visual_feat = torch.mean(visual_feat, dim=1)\n",
    "        text_feat = torch.mean(text_feat, dim=1)\n",
    "        audio_feat = torch.mean(audio_feat, dim=1)\n",
    "\n",
    "        # Concatenation of all modalities\n",
    "        fusion_out = torch.cat((visual_feat, text_feat, audio_feat), dim=-1)\n",
    "\n",
    "        # MLP Fusion\n",
    "        fusion_out = self.fusion_fc1(fusion_out)\n",
    "        fusion_out = self.fusion_fc2(fusion_out)\n",
    "        fusion_out = self.fusion_fc3(fusion_out)\n",
    "\n",
    "        # Output layer\n",
    "        output = self.output_layer(fusion_out)\n",
    "\n",
    "        return output'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-28T14:59:07.523867Z",
     "iopub.status.busy": "2025-03-28T14:59:07.523572Z",
     "iopub.status.idle": "2025-03-28T14:59:07.911744Z",
     "shell.execute_reply": "2025-03-28T14:59:07.911063Z",
     "shell.execute_reply.started": "2025-03-28T14:59:07.523844Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultimodalLSTM(\n",
       "  (visual_lstm): LSTM(1024, 512, num_layers=2, batch_first=True, bidirectional=True)\n",
       "  (text_lstm): LSTM(768, 512, num_layers=2, batch_first=True, bidirectional=True)\n",
       "  (audio_lstm): LSTM(768, 512, num_layers=2, batch_first=True, bidirectional=True)\n",
       "  (cross_attn_vt): CrossAttention(\n",
       "    (multihead_attn): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "    )\n",
       "    (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.4, inplace=False)\n",
       "  )\n",
       "  (cross_attn_va): CrossAttention(\n",
       "    (multihead_attn): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "    )\n",
       "    (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.4, inplace=False)\n",
       "  )\n",
       "  (cross_attn_ta): CrossAttention(\n",
       "    (multihead_attn): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "    )\n",
       "    (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.4, inplace=False)\n",
       "  )\n",
       "  (fusion_fc1): Sequential(\n",
       "    (0): Linear(in_features=3072, out_features=1024, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    (3): Dropout(p=0.3, inplace=False)\n",
       "  )\n",
       "  (fusion_fc2): Sequential(\n",
       "    (0): Linear(in_features=1024, out_features=512, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "    (3): Dropout(p=0.3, inplace=False)\n",
       "  )\n",
       "  (fusion_fc3): Sequential(\n",
       "    (0): Linear(in_features=512, out_features=256, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "    (3): Dropout(p=0.3, inplace=False)\n",
       "  )\n",
       "  (output_layer): Linear(in_features=256, out_features=7, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_dim = 512\n",
    "num_labels = 7\n",
    "dropout_rate = 0.3\n",
    "\n",
    "# Model\n",
    "classifier_model = MultimodalLSTM(hidden_dim=hidden_dim, num_labels=num_labels, dropout_rate=dropout_rate)\n",
    "classifier_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-28T14:59:10.961691Z",
     "iopub.status.busy": "2025-03-28T14:59:10.961326Z",
     "iopub.status.idle": "2025-03-28T14:59:10.968410Z",
     "shell.execute_reply": "2025-03-28T14:59:10.967516Z",
     "shell.execute_reply.started": "2025-03-28T14:59:10.961663Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 6.8696, 66.0000,  1.8132, 38.0535, 19.3994, 42.9940,  1.4681])\n"
     ]
    }
   ],
   "source": [
    "# Số lượng mẫu trên từng nhãn\n",
    "class_counts = torch.tensor([928, 109, 2596, 187, 358, 166, 2959])\n",
    "\n",
    "total_samples = class_counts.sum()\n",
    "\n",
    "num_negative = total_samples - class_counts\n",
    "num_positive = class_counts\n",
    "\n",
    "class_weights = num_negative / num_positive\n",
    "print(class_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-28T14:59:13.411292Z",
     "iopub.status.busy": "2025-03-28T14:59:13.410923Z",
     "iopub.status.idle": "2025-03-28T14:59:13.415748Z",
     "shell.execute_reply": "2025-03-28T14:59:13.414666Z",
     "shell.execute_reply.started": "2025-03-28T14:59:13.411265Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def get_interaction_scene(labels):\n",
    "    \n",
    "    labels_merged = []\n",
    "    for scene_label in labels:\n",
    "        \n",
    "        merged_label = np.any(scene_label, axis=0).astype(int)\n",
    "\n",
    "        labels_merged.append(merged_label)\n",
    "    \n",
    "    return labels_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-28T14:59:15.933826Z",
     "iopub.status.busy": "2025-03-28T14:59:15.933523Z",
     "iopub.status.idle": "2025-03-28T14:59:15.939789Z",
     "shell.execute_reply": "2025-03-28T14:59:15.938898Z",
     "shell.execute_reply.started": "2025-03-28T14:59:15.933804Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "criterion = nn.BCEWithLogitsLoss(pos_weight=class_weights).to(device)\n",
    "\n",
    "# Cấu hình optimizer và scheduler\n",
    "optimizer = optim.AdamW(classifier_model.parameters(), lr=0.00003, weight_decay=1e-4)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-28T14:59:25.032583Z",
     "iopub.status.busy": "2025-03-28T14:59:25.032268Z",
     "iopub.status.idle": "2025-03-28T15:05:41.603831Z",
     "shell.execute_reply": "2025-03-28T15:05:41.602882Z",
     "shell.execute_reply.started": "2025-03-28T14:59:25.032560Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/80] - Loss: 2.2283\n",
      "Macro Precision: 0.4605 - Recall: 1.0000 - F1-score: 0.5789\n",
      "Micro Precision: 0.4605 - Recall: 1.0000 - F1-score: 0.6306\n",
      "=============================================================\n",
      "Epoch [2/80] - Loss: 1.8351\n",
      "Macro Precision: 0.4605 - Recall: 1.0000 - F1-score: 0.5789\n",
      "Micro Precision: 0.4605 - Recall: 1.0000 - F1-score: 0.6306\n",
      "=============================================================\n",
      "Epoch [3/80] - Loss: 1.8093\n",
      "Macro Precision: 0.4605 - Recall: 1.0000 - F1-score: 0.5789\n",
      "Micro Precision: 0.4605 - Recall: 1.0000 - F1-score: 0.6306\n",
      "=============================================================\n",
      "Epoch [4/80] - Loss: 1.7913\n",
      "Macro Precision: 0.4605 - Recall: 1.0000 - F1-score: 0.5789\n",
      "Micro Precision: 0.4605 - Recall: 1.0000 - F1-score: 0.6306\n",
      "=============================================================\n",
      "Epoch [5/80] - Loss: 1.8113\n",
      "Macro Precision: 0.4605 - Recall: 1.0000 - F1-score: 0.5789\n",
      "Micro Precision: 0.4605 - Recall: 1.0000 - F1-score: 0.6306\n",
      "=============================================================\n",
      "Epoch [6/80] - Loss: 1.7431\n",
      "Macro Precision: 0.4605 - Recall: 1.0000 - F1-score: 0.5789\n",
      "Micro Precision: 0.4605 - Recall: 1.0000 - F1-score: 0.6306\n",
      "=============================================================\n",
      "Epoch [7/80] - Loss: 1.7508\n",
      "Macro Precision: 0.4623 - Recall: 0.9847 - F1-score: 0.5798\n",
      "Micro Precision: 0.4668 - Recall: 0.9917 - F1-score: 0.6348\n",
      "=============================================================\n",
      "Epoch [8/80] - Loss: 1.7208\n",
      "Macro Precision: 0.4658 - Recall: 0.9847 - F1-score: 0.5839\n",
      "Micro Precision: 0.4717 - Recall: 0.9917 - F1-score: 0.6393\n",
      "=============================================================\n",
      "Epoch [9/80] - Loss: 1.7219\n",
      "Macro Precision: 0.4658 - Recall: 0.9847 - F1-score: 0.5839\n",
      "Micro Precision: 0.4717 - Recall: 0.9917 - F1-score: 0.6393\n",
      "=============================================================\n",
      "Epoch [10/80] - Loss: 1.7094\n",
      "Macro Precision: 0.4623 - Recall: 0.9847 - F1-score: 0.5798\n",
      "Micro Precision: 0.4668 - Recall: 0.9917 - F1-score: 0.6348\n",
      "=============================================================\n",
      "Epoch [11/80] - Loss: 1.6916\n",
      "Macro Precision: 0.4822 - Recall: 0.9796 - F1-score: 0.5972\n",
      "Micro Precision: 0.4917 - Recall: 0.9889 - F1-score: 0.6569\n",
      "=============================================================\n",
      "Epoch [12/80] - Loss: 1.7141\n",
      "Macro Precision: 0.4638 - Recall: 0.9898 - F1-score: 0.5826\n",
      "Micro Precision: 0.4705 - Recall: 0.9945 - F1-score: 0.6388\n",
      "=============================================================\n",
      "Epoch [13/80] - Loss: 1.6394\n",
      "Macro Precision: 0.4836 - Recall: 0.9847 - F1-score: 0.5990\n",
      "Micro Precision: 0.4924 - Recall: 0.9917 - F1-score: 0.6581\n",
      "=============================================================\n",
      "Epoch [14/80] - Loss: 1.6552\n",
      "Macro Precision: 0.4775 - Recall: 0.9898 - F1-score: 0.5933\n",
      "Micro Precision: 0.4832 - Recall: 0.9945 - F1-score: 0.6504\n",
      "=============================================================\n",
      "Epoch [15/80] - Loss: 1.6393\n",
      "Macro Precision: 0.4863 - Recall: 0.9847 - F1-score: 0.6015\n",
      "Micro Precision: 0.4965 - Recall: 0.9917 - F1-score: 0.6617\n",
      "=============================================================\n",
      "Epoch [16/80] - Loss: 1.6193\n",
      "Macro Precision: 0.4855 - Recall: 0.9898 - F1-score: 0.6017\n",
      "Micro Precision: 0.4952 - Recall: 0.9945 - F1-score: 0.6611\n",
      "=============================================================\n",
      "Epoch [17/80] - Loss: 1.5893\n",
      "Macro Precision: 0.4783 - Recall: 0.9949 - F1-score: 0.5960\n",
      "Micro Precision: 0.4858 - Recall: 0.9972 - F1-score: 0.6534\n",
      "=============================================================\n",
      "Epoch [18/80] - Loss: 1.5825\n",
      "Macro Precision: 0.5048 - Recall: 0.9474 - F1-score: 0.6149\n",
      "Micro Precision: 0.5321 - Recall: 0.9640 - F1-score: 0.6857\n",
      "=============================================================\n",
      "Epoch [19/80] - Loss: 1.5637\n",
      "Macro Precision: 0.4773 - Recall: 0.9949 - F1-score: 0.5952\n",
      "Micro Precision: 0.4852 - Recall: 0.9972 - F1-score: 0.6528\n",
      "=============================================================\n",
      "Epoch [20/80] - Loss: 1.6861\n",
      "Macro Precision: 0.4685 - Recall: 0.9847 - F1-score: 0.5890\n",
      "Micro Precision: 0.4844 - Recall: 0.9917 - F1-score: 0.6509\n",
      "=============================================================\n",
      "Epoch [21/80] - Loss: 1.6221\n",
      "Macro Precision: 0.4849 - Recall: 0.9847 - F1-score: 0.5998\n",
      "Micro Precision: 0.4938 - Recall: 0.9917 - F1-score: 0.6593\n",
      "=============================================================\n",
      "Epoch [22/80] - Loss: 1.5567\n",
      "Macro Precision: 0.4928 - Recall: 0.9794 - F1-score: 0.6071\n",
      "Micro Precision: 0.5057 - Recall: 0.9889 - F1-score: 0.6692\n",
      "=============================================================\n",
      "Epoch [23/80] - Loss: 1.5510\n",
      "Macro Precision: 0.4664 - Recall: 1.0000 - F1-score: 0.5862\n",
      "Micro Precision: 0.4719 - Recall: 1.0000 - F1-score: 0.6412\n",
      "=============================================================\n",
      "Epoch [24/80] - Loss: 1.7142\n",
      "Macro Precision: 0.4911 - Recall: 0.9845 - F1-score: 0.6050\n",
      "Micro Precision: 0.5000 - Recall: 0.9917 - F1-score: 0.6648\n",
      "=============================================================\n",
      "Epoch [25/80] - Loss: 1.5321\n",
      "Macro Precision: 0.4983 - Recall: 0.9705 - F1-score: 0.6120\n",
      "Micro Precision: 0.5175 - Recall: 0.9806 - F1-score: 0.6775\n",
      "=============================================================\n",
      "Epoch [26/80] - Loss: 1.4198\n",
      "Macro Precision: 0.4820 - Recall: 0.9845 - F1-score: 0.5993\n",
      "Micro Precision: 0.4952 - Recall: 0.9917 - F1-score: 0.6605\n",
      "=============================================================\n",
      "Epoch [27/80] - Loss: 1.3918\n",
      "Macro Precision: 0.5455 - Recall: 0.8814 - F1-score: 0.6371\n",
      "Micro Precision: 0.5971 - Recall: 0.9197 - F1-score: 0.7241\n",
      "=============================================================\n",
      "Epoch [28/80] - Loss: 1.3155\n",
      "Macro Precision: 0.5165 - Recall: 0.9243 - F1-score: 0.6278\n",
      "Micro Precision: 0.5644 - Recall: 0.9584 - F1-score: 0.7105\n",
      "=============================================================\n",
      "Epoch [29/80] - Loss: 1.3545\n",
      "Macro Precision: 0.5435 - Recall: 0.8899 - F1-score: 0.6348\n",
      "Micro Precision: 0.5873 - Recall: 0.9224 - F1-score: 0.7177\n",
      "=============================================================\n",
      "Epoch [30/80] - Loss: 1.2930\n",
      "Macro Precision: 0.5246 - Recall: 0.8878 - F1-score: 0.6306\n",
      "Micro Precision: 0.5878 - Recall: 0.9363 - F1-score: 0.7222\n",
      "=============================================================\n",
      "Epoch [31/80] - Loss: 1.3076\n",
      "Macro Precision: 0.4893 - Recall: 0.9746 - F1-score: 0.6068\n",
      "Micro Precision: 0.5108 - Recall: 0.9861 - F1-score: 0.6730\n",
      "=============================================================\n",
      "Epoch [32/80] - Loss: 1.2326\n",
      "Macro Precision: 0.4974 - Recall: 0.9746 - F1-score: 0.6159\n",
      "Micro Precision: 0.5251 - Recall: 0.9861 - F1-score: 0.6853\n",
      "=============================================================\n",
      "Epoch [33/80] - Loss: 1.1863\n",
      "Macro Precision: 0.5073 - Recall: 0.9194 - F1-score: 0.6187\n",
      "Micro Precision: 0.5538 - Recall: 0.9557 - F1-score: 0.7012\n",
      "=============================================================\n",
      "Epoch [34/80] - Loss: 1.2067\n",
      "Macro Precision: 0.5324 - Recall: 0.9464 - F1-score: 0.6382\n",
      "Micro Precision: 0.5659 - Recall: 0.9640 - F1-score: 0.7131\n",
      "=============================================================\n",
      "Epoch [35/80] - Loss: 1.1045\n",
      "Macro Precision: 0.5279 - Recall: 0.8783 - F1-score: 0.6337\n",
      "Micro Precision: 0.5971 - Recall: 0.9280 - F1-score: 0.7267\n",
      "=============================================================\n",
      "Epoch [36/80] - Loss: 1.0453\n",
      "Macro Precision: 0.5235 - Recall: 0.8837 - F1-score: 0.6315\n",
      "Micro Precision: 0.5881 - Recall: 0.9335 - F1-score: 0.7216\n",
      "=============================================================\n",
      "Epoch [37/80] - Loss: 1.0219\n",
      "Macro Precision: 0.5020 - Recall: 0.9236 - F1-score: 0.6188\n",
      "Micro Precision: 0.5527 - Recall: 0.9584 - F1-score: 0.7011\n",
      "=============================================================\n",
      "Epoch [38/80] - Loss: 0.9750\n",
      "Macro Precision: 0.5078 - Recall: 0.9520 - F1-score: 0.6246\n",
      "Micro Precision: 0.5476 - Recall: 0.9723 - F1-score: 0.7006\n",
      "=============================================================\n",
      "Epoch [39/80] - Loss: 0.9978\n",
      "Macro Precision: 0.5768 - Recall: 0.8550 - F1-score: 0.6664\n",
      "Micro Precision: 0.6587 - Recall: 0.9141 - F1-score: 0.7657\n",
      "=============================================================\n",
      "Epoch [40/80] - Loss: 0.8549\n",
      "Macro Precision: 0.5288 - Recall: 0.8868 - F1-score: 0.6356\n",
      "Micro Precision: 0.5892 - Recall: 0.9335 - F1-score: 0.7224\n",
      "=============================================================\n",
      "Epoch [41/80] - Loss: 0.8598\n",
      "Macro Precision: 0.5247 - Recall: 0.9396 - F1-score: 0.6401\n",
      "Micro Precision: 0.5733 - Recall: 0.9640 - F1-score: 0.7190\n",
      "=============================================================\n",
      "Epoch [42/80] - Loss: 0.8479\n",
      "Macro Precision: 0.5537 - Recall: 0.8592 - F1-score: 0.6571\n",
      "Micro Precision: 0.6411 - Recall: 0.9252 - F1-score: 0.7574\n",
      "=============================================================\n",
      "Epoch [43/80] - Loss: 0.7808\n",
      "Macro Precision: 0.5181 - Recall: 0.9067 - F1-score: 0.6330\n",
      "Micro Precision: 0.5755 - Recall: 0.9501 - F1-score: 0.7168\n",
      "=============================================================\n",
      "Epoch [44/80] - Loss: 0.7114\n",
      "Macro Precision: 0.5628 - Recall: 0.7565 - F1-score: 0.6291\n",
      "Micro Precision: 0.6753 - Recall: 0.8643 - F1-score: 0.7582\n",
      "=============================================================\n",
      "Epoch [45/80] - Loss: 0.6566\n",
      "Macro Precision: 0.5621 - Recall: 0.8493 - F1-score: 0.6613\n",
      "Micro Precision: 0.6393 - Recall: 0.9280 - F1-score: 0.7571\n",
      "=============================================================\n",
      "Epoch [46/80] - Loss: 0.6174\n",
      "Macro Precision: 0.5796 - Recall: 0.7319 - F1-score: 0.6397\n",
      "Micro Precision: 0.6904 - Recall: 0.8587 - F1-score: 0.7654\n",
      "=============================================================\n",
      "Epoch [47/80] - Loss: 0.5824\n",
      "Macro Precision: 0.5603 - Recall: 0.6849 - F1-score: 0.6089\n",
      "Micro Precision: 0.7167 - Recall: 0.8338 - F1-score: 0.7708\n",
      "=============================================================\n",
      "Epoch [48/80] - Loss: 0.5494\n",
      "Macro Precision: 0.5487 - Recall: 0.7497 - F1-score: 0.6241\n",
      "Micro Precision: 0.6702 - Recall: 0.8726 - F1-score: 0.7581\n",
      "=============================================================\n",
      "Epoch [49/80] - Loss: 0.4731\n",
      "Macro Precision: 0.5633 - Recall: 0.7234 - F1-score: 0.6281\n",
      "Micro Precision: 0.6834 - Recall: 0.8670 - F1-score: 0.7643\n",
      "=============================================================\n",
      "Epoch [50/80] - Loss: 0.4513\n",
      "Macro Precision: 0.5928 - Recall: 0.6355 - F1-score: 0.6027\n",
      "Micro Precision: 0.7742 - Recall: 0.7978 - F1-score: 0.7858\n",
      "=============================================================\n",
      "Epoch [51/80] - Loss: 0.4299\n",
      "Macro Precision: 0.5536 - Recall: 0.8134 - F1-score: 0.6403\n",
      "Micro Precision: 0.6533 - Recall: 0.9030 - F1-score: 0.7581\n",
      "=============================================================\n",
      "Epoch [52/80] - Loss: 0.4602\n",
      "Macro Precision: 0.5662 - Recall: 0.7856 - F1-score: 0.6489\n",
      "Micro Precision: 0.6626 - Recall: 0.8920 - F1-score: 0.7603\n",
      "=============================================================\n",
      "Epoch [53/80] - Loss: 0.3984\n",
      "Macro Precision: 0.5912 - Recall: 0.6620 - F1-score: 0.6211\n",
      "Micro Precision: 0.7468 - Recall: 0.8172 - F1-score: 0.7804\n",
      "=============================================================\n",
      "Epoch [54/80] - Loss: 0.3405\n",
      "Macro Precision: 0.5772 - Recall: 0.6019 - F1-score: 0.5807\n",
      "Micro Precision: 0.7613 - Recall: 0.7950 - F1-score: 0.7778\n",
      "=============================================================\n",
      "Epoch [55/80] - Loss: 0.3243\n",
      "Macro Precision: 0.5604 - Recall: 0.5980 - F1-score: 0.5746\n",
      "Micro Precision: 0.7442 - Recall: 0.7978 - F1-score: 0.7701\n",
      "=============================================================\n",
      "Epoch [56/80] - Loss: 0.3159\n",
      "Macro Precision: 0.5752 - Recall: 0.6182 - F1-score: 0.5905\n",
      "Micro Precision: 0.7430 - Recall: 0.8089 - F1-score: 0.7745\n",
      "=============================================================\n",
      "Epoch [57/80] - Loss: 0.2911\n",
      "Macro Precision: 0.5798 - Recall: 0.6604 - F1-score: 0.6137\n",
      "Micro Precision: 0.7282 - Recall: 0.8310 - F1-score: 0.7762\n",
      "=============================================================\n",
      "Epoch [58/80] - Loss: 0.2857\n",
      "Macro Precision: 0.5745 - Recall: 0.6122 - F1-score: 0.5864\n",
      "Micro Precision: 0.7500 - Recall: 0.8061 - F1-score: 0.7770\n",
      "=============================================================\n",
      "Epoch [59/80] - Loss: 0.2763\n",
      "Macro Precision: 0.5860 - Recall: 0.6053 - F1-score: 0.5884\n",
      "Micro Precision: 0.7605 - Recall: 0.8006 - F1-score: 0.7800\n",
      "=============================================================\n",
      "Epoch [60/80] - Loss: 0.2633\n",
      "Macro Precision: 0.5696 - Recall: 0.6538 - F1-score: 0.5955\n",
      "Micro Precision: 0.7443 - Recall: 0.8144 - F1-score: 0.7778\n",
      "=============================================================\n",
      "Epoch [61/80] - Loss: 0.2715\n",
      "Macro Precision: 0.5705 - Recall: 0.6131 - F1-score: 0.5864\n",
      "Micro Precision: 0.7462 - Recall: 0.8061 - F1-score: 0.7750\n",
      "=============================================================\n",
      "Epoch [62/80] - Loss: 0.2499\n",
      "Macro Precision: 0.6007 - Recall: 0.6291 - F1-score: 0.6130\n",
      "Micro Precision: 0.7618 - Recall: 0.8061 - F1-score: 0.7833\n",
      "=============================================================\n",
      "Epoch [63/80] - Loss: 0.2359\n",
      "Macro Precision: 0.5746 - Recall: 0.6189 - F1-score: 0.5920\n",
      "Micro Precision: 0.7539 - Recall: 0.8061 - F1-score: 0.7791\n",
      "=============================================================\n",
      "Epoch [64/80] - Loss: 0.2410\n",
      "Macro Precision: 0.5831 - Recall: 0.5580 - F1-score: 0.5574\n",
      "Micro Precision: 0.7815 - Recall: 0.7729 - F1-score: 0.7772\n",
      "=============================================================\n",
      "Epoch [65/80] - Loss: 0.2361\n",
      "Macro Precision: 0.5652 - Recall: 0.5750 - F1-score: 0.5643\n",
      "Micro Precision: 0.7560 - Recall: 0.7895 - F1-score: 0.7724\n",
      "=============================================================\n",
      "Epoch [66/80] - Loss: 0.2234\n",
      "Macro Precision: 0.5735 - Recall: 0.6248 - F1-score: 0.5939\n",
      "Micro Precision: 0.7468 - Recall: 0.8089 - F1-score: 0.7766\n",
      "=============================================================\n",
      "Epoch [67/80] - Loss: 0.2385\n",
      "Macro Precision: 0.5490 - Recall: 0.6607 - F1-score: 0.5888\n",
      "Micro Precision: 0.7202 - Recall: 0.8199 - F1-score: 0.7668\n",
      "=============================================================\n",
      "Epoch [68/80] - Loss: 0.2164\n",
      "Macro Precision: 0.5769 - Recall: 0.6004 - F1-score: 0.5836\n",
      "Micro Precision: 0.7494 - Recall: 0.8033 - F1-score: 0.7754\n",
      "=============================================================\n",
      "Epoch [69/80] - Loss: 0.2097\n",
      "Macro Precision: 0.5849 - Recall: 0.5652 - F1-score: 0.5646\n",
      "Micro Precision: 0.7849 - Recall: 0.7784 - F1-score: 0.7816\n",
      "=============================================================\n",
      "Epoch [70/80] - Loss: 0.1982\n",
      "Macro Precision: 0.5723 - Recall: 0.6340 - F1-score: 0.5910\n",
      "Micro Precision: 0.7474 - Recall: 0.8116 - F1-score: 0.7782\n",
      "=============================================================\n",
      "Epoch [71/80] - Loss: 0.1937\n",
      "Macro Precision: 0.5724 - Recall: 0.5697 - F1-score: 0.5607\n",
      "Micro Precision: 0.7732 - Recall: 0.7839 - F1-score: 0.7785\n",
      "=============================================================\n",
      "Epoch [72/80] - Loss: 0.1856\n",
      "Macro Precision: 0.5688 - Recall: 0.6095 - F1-score: 0.5843\n",
      "Micro Precision: 0.7513 - Recall: 0.8033 - F1-score: 0.7764\n",
      "=============================================================\n",
      "Epoch [73/80] - Loss: 0.1859\n",
      "Macro Precision: 0.5899 - Recall: 0.5879 - F1-score: 0.5814\n",
      "Micro Precision: 0.7766 - Recall: 0.7895 - F1-score: 0.7830\n",
      "=============================================================\n",
      "Epoch [74/80] - Loss: 0.1840\n",
      "Macro Precision: 0.5756 - Recall: 0.6000 - F1-score: 0.5835\n",
      "Micro Precision: 0.7559 - Recall: 0.7978 - F1-score: 0.7763\n",
      "=============================================================\n",
      "Epoch [75/80] - Loss: 0.1758\n",
      "Macro Precision: 0.5858 - Recall: 0.6304 - F1-score: 0.6013\n",
      "Micro Precision: 0.7519 - Recall: 0.8144 - F1-score: 0.7819\n",
      "=============================================================\n",
      "Epoch [76/80] - Loss: 0.1677\n",
      "Macro Precision: 0.5710 - Recall: 0.5569 - F1-score: 0.5503\n",
      "Micro Precision: 0.7827 - Recall: 0.7784 - F1-score: 0.7806\n",
      "=============================================================\n",
      "Epoch [77/80] - Loss: 0.1694\n",
      "Macro Precision: 0.5671 - Recall: 0.6044 - F1-score: 0.5794\n",
      "Micro Precision: 0.7560 - Recall: 0.7895 - F1-score: 0.7724\n",
      "=============================================================\n",
      "Epoch [78/80] - Loss: 0.1688\n",
      "Macro Precision: 0.5955 - Recall: 0.5870 - F1-score: 0.5807\n",
      "Micro Precision: 0.7613 - Recall: 0.7950 - F1-score: 0.7778\n",
      "=============================================================\n",
      "Epoch [79/80] - Loss: 0.1630\n",
      "Macro Precision: 0.5845 - Recall: 0.5911 - F1-score: 0.5831\n",
      "Micro Precision: 0.7696 - Recall: 0.7867 - F1-score: 0.7781\n",
      "=============================================================\n",
      "Epoch [80/80] - Loss: 0.1564\n",
      "Macro Precision: 0.5938 - Recall: 0.6552 - F1-score: 0.6046\n",
      "Micro Precision: 0.7644 - Recall: 0.8089 - F1-score: 0.7860\n",
      "=============================================================\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 60\n",
    "save_dir = \"/kaggle/working/\"\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    classifier_model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for visual_data, text_data, audio_data, labels in train_feats_loader:\n",
    "        # Chuyển dữ liệu sang thiết bị\n",
    "        visual_data, text_data, audio_data, labels = visual_data.to(device), text_data.to(device), audio_data.to(device), labels.to(device)\n",
    "        \n",
    "        # Hợp nhất nhãn cho toàn bộ scene\n",
    "        scene_labels = np.array(get_interaction_scene(labels.cpu().numpy()))  # Chuyển thành numpy array\n",
    "        labels = torch.from_numpy(scene_labels).float().to(device)  # Chuyển thành tensor và đưa vào device\n",
    "       \n",
    "        optimizer.zero_grad()\n",
    "        outputs = classifier_model(visual_data, text_data, audio_data)\n",
    "        \n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}] - Loss: {running_loss / len(train_feats_loader):.4f}\")\n",
    "\n",
    "    # ============================= Validation ==============================\n",
    "    classifier_model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_preds = []\n",
    "    val_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for visual_data, text_data, audio_data, labels in val_feats_loader:\n",
    "            # Chuyển dữ liệu sang thiết bị\n",
    "            visual_data, text_data, audio_data, labels = visual_data.to(device), text_data.to(device), audio_data.to(device), labels.to(device)\n",
    "            \n",
    "            # Hợp nhất nhãn\n",
    "            scene_labels = np.array(get_interaction_scene(labels.cpu().numpy()))\n",
    "            labels = torch.from_numpy(scene_labels).float().to(device)\n",
    "           \n",
    "            outputs = classifier_model(visual_data, text_data, audio_data)\n",
    "\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "            outputs = torch.sigmoid(outputs)\n",
    "            \n",
    "            # Lưu dự đoán và nhãn\n",
    "            val_preds.append(outputs.cpu().numpy())  # Store predictions\n",
    "            val_labels.append(labels.cpu().numpy())  # Store ground truth\n",
    "\n",
    "    # Convert lists of predictions and labels into numpy arrays\n",
    "    val_preds = np.concatenate(val_preds, axis=0)\n",
    "    val_labels = np.concatenate(val_labels, axis=0)\n",
    "    \n",
    "    # Convert logits to binary predictions using a threshold of 0.5\n",
    "    val_preds = (val_preds > 0.6).astype(int)\n",
    "    val_labels = val_labels.astype(int)\n",
    "\n",
    "    # Tính các chỉ số đánh giá\n",
    "    # Macro Precision, Recall, F1\n",
    "    precision_macro = precision_score(val_labels, val_preds, average='macro', zero_division=0)\n",
    "    recall_macro = recall_score(val_labels, val_preds, average='macro', zero_division=0)\n",
    "    f1_macro = f1_score(val_labels, val_preds, average='macro', zero_division=0)\n",
    "    \n",
    "    # Micro Precision, Recall, F1\n",
    "    precision_micro = precision_score(val_labels, val_preds, average='micro', zero_division=0)\n",
    "    recall_micro = recall_score(val_labels, val_preds, average='micro', zero_division=0)\n",
    "    f1_micro = f1_score(val_labels, val_preds, average='micro', zero_division=0)\n",
    "   \n",
    "\n",
    "    print(f\"Macro Precision: {precision_macro:.4f} - Recall: {recall_macro:.4f} - F1-score: {f1_macro:.4f}\")\n",
    "    print(f\"Micro Precision: {precision_micro:.4f} - Recall: {recall_micro:.4f} - F1-score: {f1_micro:.4f}\")\n",
    "    print(\"=============================================================\")\n",
    "\n",
    "torch.save(classifier_model.state_dict(), f\"{save_dir}best_model.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "classifier_model.load_state_dict(torch.load('/kaggle/input/checkpoint/best_model_3f.pth', weights_only=True))\n",
    "classifier_model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate in test set in scene level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-28T15:12:05.807756Z",
     "iopub.status.busy": "2025-03-28T15:12:05.807266Z",
     "iopub.status.idle": "2025-03-28T15:12:07.644975Z",
     "shell.execute_reply": "2025-03-28T15:12:07.643911Z",
     "shell.execute_reply.started": "2025-03-28T15:12:05.807725Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grouped features saved\n"
     ]
    }
   ],
   "source": [
    "features_path = \"/kaggle/input/checkpoint/full_v_t_a/test_full_v_t_a.pkl\"\n",
    "test_video_paths, test_visual_features, test_text_paths, test_text_features, test_audio_paths, test_audio_features, test_labels = load_features(features_path)\n",
    "test_scene_feats = group_features(test_video_paths, test_visual_features, test_text_paths, test_text_features, test_audio_paths, test_audio_features, test_labels)\n",
    "\n",
    "with open(\"/kaggle/working/grouped_test_features.pkl\", \"wb\") as f:\n",
    "    pickle.dump(test_scene_feats, f)\n",
    "print(f\"Grouped features saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-28T15:12:09.851860Z",
     "iopub.status.busy": "2025-03-28T15:12:09.851550Z",
     "iopub.status.idle": "2025-03-28T15:12:09.858299Z",
     "shell.execute_reply": "2025-03-28T15:12:09.857525Z",
     "shell.execute_reply.started": "2025-03-28T15:12:09.851836Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "for scene_name, scene_data in test_scene_feats.items():\n",
    "    scene_data['labels'] = F.one_hot(scene_data['labels'], num_classes=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-28T15:12:11.789339Z",
     "iopub.status.busy": "2025-03-28T15:12:11.788978Z",
     "iopub.status.idle": "2025-03-28T15:12:11.815201Z",
     "shell.execute_reply": "2025-03-28T15:12:11.814444Z",
     "shell.execute_reply.started": "2025-03-28T15:12:11.789312Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "test_feats_padded = pad_scene_level(test_scene_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-28T15:12:13.665069Z",
     "iopub.status.busy": "2025-03-28T15:12:13.664723Z",
     "iopub.status.idle": "2025-03-28T15:12:13.682689Z",
     "shell.execute_reply": "2025-03-28T15:12:13.681878Z",
     "shell.execute_reply.started": "2025-03-28T15:12:13.665044Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "visual_features = []\n",
    "text_features = []\n",
    "audio_features = []\n",
    "labels = []\n",
    "scene_names = []\n",
    "\n",
    "for scene_name, scene in test_feats_padded.items():\n",
    "    scene_names.append(scene_name)\n",
    "    visual_features.append(scene['visual_features'])\n",
    "    text_features.append(scene['text_features'])\n",
    "    audio_features.append(scene['audio_features'])\n",
    "    labels.append(scene['labels'])\n",
    "\n",
    "# Chuyển các list thành tensor\n",
    "visual_features = torch.stack(visual_features)\n",
    "text_features = torch.stack(text_features)\n",
    "audio_features = torch.stack(audio_features)\n",
    "labels = torch.stack(labels)\n",
    "\n",
    "# Tạo TensorDataset từ các đặc trưng visual, text và labels\n",
    "test_feats = TensorDataset(visual_features, text_features, audio_features, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-28T15:12:15.967587Z",
     "iopub.status.busy": "2025-03-28T15:12:15.967298Z",
     "iopub.status.idle": "2025-03-28T15:12:15.972505Z",
     "shell.execute_reply": "2025-03-28T15:12:15.971685Z",
     "shell.execute_reply.started": "2025-03-28T15:12:15.967565Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: 131 scenes\n"
     ]
    }
   ],
   "source": [
    "test_feats_loader = DataLoader(test_feats, batch_size=16, shuffle=False)\n",
    "print(f\"Test set: {len(test_feats_loader.dataset)} scenes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-28T15:12:18.546014Z",
     "iopub.status.busy": "2025-03-28T15:12:18.545633Z",
     "iopub.status.idle": "2025-03-28T15:12:18.974840Z",
     "shell.execute_reply": "2025-03-28T15:12:18.973740Z",
     "shell.execute_reply.started": "2025-03-28T15:12:18.545983Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'for i, scene_name in enumerate(scene_names):\\n    print(f\"Scene Name: {scene_name}\")\\n    print(f\"Prediction: {test_preds[i]}\")\\n    print(f\"Ground Truth: {test_labels[i]}\")\\n    print(\"===================================\")'"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_preds = []\n",
    "test_labels = []\n",
    "\n",
    "classifier_model.eval()\n",
    "\n",
    "with torch.no_grad(): \n",
    "    #for visual_data, text_data, audio_data, labels in test_feats_loader:\n",
    "    for idx, (visual_data, text_data, audio_data, labels) in enumerate(test_feats_loader):\n",
    "        \n",
    "        visual_data, text_data, audio_data, labels = visual_data.to(device), text_data.to(device), audio_data.to(device), labels.to(device)\n",
    "        \n",
    "        scene_labels = np.array(get_interaction_scene(labels.cpu().numpy()))  # Hợp nhất nhãn\n",
    "        labels = torch.from_numpy(scene_labels).float().to(device)  # Chuyển thành tensor và đưa vào device\n",
    "\n",
    "        outputs = classifier_model(visual_data, text_data, audio_data)\n",
    "        outputs = torch.sigmoid(outputs)\n",
    "        test_preds.append(outputs.cpu().numpy())  \n",
    "        test_labels.append(labels.cpu().numpy())\n",
    "\n",
    "# Chuyển các danh sách về numpy arrays\n",
    "test_preds = np.concatenate(test_preds, axis=0)\n",
    "test_labels = np.concatenate(test_labels, axis=0)\n",
    "\n",
    "test_preds_scene = (test_preds > 0.6).astype(int)\n",
    "test_labels_scene = test_labels.astype(int)\n",
    "\n",
    "'''for i, scene_name in enumerate(scene_names):\n",
    "    print(f\"Scene Name: {scene_name}\")\n",
    "    print(f\"Prediction: {test_preds[i]}\")\n",
    "    print(f\"Ground Truth: {test_labels[i]}\")\n",
    "    print(\"===================================\")'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-28T15:12:22.010399Z",
     "iopub.status.busy": "2025-03-28T15:12:22.010080Z",
     "iopub.status.idle": "2025-03-28T15:12:22.026794Z",
     "shell.execute_reply": "2025-03-28T15:12:22.025927Z",
     "shell.execute_reply.started": "2025-03-28T15:12:22.010375Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Macro Precision: 0.5542 - Recall: 0.6201 - F1-score: 0.5694\n",
      "Test Micro Precision: 0.7158 - Recall: 0.8088 - F1-score: 0.7595\n"
     ]
    }
   ],
   "source": [
    "#Tính các chỉ số đánh giá trên bộ test\n",
    "precision_macro = precision_score(test_labels_scene, test_preds_scene, average='macro')\n",
    "recall_macro = recall_score(test_labels_scene, test_preds_scene, average='macro')\n",
    "f1_macro = f1_score(test_labels_scene, test_preds_scene, average='macro')\n",
    "\n",
    "precision_micro = precision_score(test_labels_scene, test_preds_scene, average='micro')\n",
    "recall_micro = recall_score(test_labels_scene, test_preds_scene, average='micro')\n",
    "f1_micro = f1_score(test_labels_scene, test_preds_scene, average='micro')\n",
    "\n",
    "# In kết quả\n",
    "print(f\"Test Macro Precision: {precision_macro:.4f} - Recall: {recall_macro:.4f} - F1-score: {f1_macro:.4f}\")\n",
    "print(f\"Test Micro Precision: {precision_micro:.4f} - Recall: {recall_micro:.4f} - F1-score: {f1_micro:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 6084042,
     "sourceId": 10073534,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6344361,
     "sourceId": 10256008,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6313490,
     "sourceId": 10263245,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6387485,
     "sourceId": 10317534,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6407414,
     "sourceId": 10347384,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6346885,
     "sourceId": 10862917,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30823,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
